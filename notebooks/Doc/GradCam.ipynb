{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "project_dir = os.path.join(os.getcwd(),'../..')\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "attention_dir = os.path.join(project_dir, 'modules/AttentionMap')\n",
    "if attention_dir not in sys.path:\n",
    "    sys.path.append(attention_dir)\n",
    "\n",
    "sparse_dir = os.path.join(project_dir, 'modules/Sparse')\n",
    "if sparse_dir not in sys.path:\n",
    "    sys.path.append(sparse_dir) \n",
    "\n",
    "import numpy as np\n",
    "import torch, config\n",
    "from torch import nn\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, ToTensor, ToPILImage, Normalize, Resize\n",
    "from derma.utils import UnNormalize\n",
    "from derma.dataset import Derma\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = Compose([\n",
    "        Resize((256, 256)),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "to_pil = Compose([\n",
    "        UnNormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToPILImage()\n",
    "    ])\n",
    "\n",
    "dataset_dir = os.path.join(config.DATASET_DIR, 'test')\n",
    "dataset = Derma(dataset_dir, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradCAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generar un dataframe con el nombre de los ficheros por indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = list( map(lambda x: os.path.join(os.path.dirname(x)[-1], os.path.split(x)[1]), dataset.x) )\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from derma.doc.utils import GradCamAttribute\n",
    "\n",
    "def ObtainGradCam(model, loader):\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "\n",
    "    grad_cam = None\n",
    "\n",
    "    for inputs, targets in loader:\n",
    "        inputs.requires_grad = True\n",
    "\n",
    "        attribution = GradCamAttribute(model, model.features[-1][0], inputs, targets)\n",
    "        attribution = attribution.mean(axis=1).abs().detach() # Remove negative values\n",
    "\n",
    "        grad_cam = torch.concat([grad_cam, attribution], axis=0) if grad_cam is not None else attribution\n",
    "\n",
    "    return grad_cam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Obtener los GradCAM con el conjunto de test y almacenarlo en disco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from derma.architecture import InvertedResidual\n",
    "from torchvision.models import MobileNetV2\n",
    "from derma.doc.utils import summary\n",
    "\n",
    "experiment = config.experiment\n",
    "for idx, model_name in enumerate(experiment.keys()):\n",
    "    weights_dir = os.path.join(config.RESULT_DIR, 'weights/classification/{}/HAM10000/model.pth'.format(model_name))\n",
    "    net_config = experiment[model_name]\n",
    "    model = MobileNetV2(num_classes=2, inverted_residual_setting=net_config['inverted_residual_setting'],\n",
    "                        block=InvertedResidual if net_config['attention'] else None)\n",
    "\n",
    "    model.load_state_dict(torch.load(weights_dir))\n",
    "\n",
    "    grad_cam_save_dir = os.path.join(config.RESULT_DIR, 'grad_cam/HAM10000/{}/'.format(model_name))\n",
    "    if not os.path.exists(grad_cam_save_dir):\n",
    "        os.makedirs(grad_cam_save_dir)\n",
    "\n",
    "    grad_cam = ObtainGradCam(model, loader)\n",
    "    torch.save(grad_cam, os.path.join(grad_cam_save_dir, 'grad_cam.pth'))\n",
    "    df.to_csv(os.path.join(grad_cam_save_dir, 'images_name.csv'), index=True, header=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_result_dir =  os.path.join(config.RESULT_DIR, 'grad_cam/HAM10000/')\n",
    "dataset_dir = os.path.join(config.DATASET_DIR, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from derma.doc.utils import plot_attribution\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "exp = list(config.experiment.keys())\n",
    "for model_name in exp:\n",
    "    grad_cam_model_dir = os.path.join(grad_cam_result_dir, model_name)\n",
    "    images_name = pd.read_csv(os.path.join(grad_cam_model_dir, 'images_name.csv'), header=None, index_col=0)\n",
    "    gradcam_att = torch.load(os.path.join(grad_cam_model_dir, 'grad_cam.pth'))\n",
    "\n",
    "    img_save_dir = os.path.join(grad_cam_model_dir, 'imgs')\n",
    "\n",
    "    for idx in range(len(images_name)):\n",
    "        filename = images_name.iloc[idx].values[0]\n",
    "        target, name =  os.path.split(filename)\n",
    "        name, ext = os.path.splitext(name)\n",
    "        full_path_filename = os.path.join(dataset_dir, filename)\n",
    "\n",
    "        img = Image.open(full_path_filename).convert('RGB').resize((256, 256))\n",
    "        att = gradcam_att[idx].numpy()\n",
    "        try:\n",
    "            fig = plot_attribution(att, img)\n",
    "            current_save_dir = os.path.join(img_save_dir, target)\n",
    "            fig.savefig(os.path.join(current_save_dir, '{}.pdf'.format(name)), bbox_inches = 'tight', pad_inches = 0)\n",
    "        except:\n",
    "            print(\"An exception occurred: {}\".format(filename)) \n",
    "\n",
    "        plt.close(fig)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d9b8aa8d774518be7ebcfd06a2463a8035a66798fac49b1a363f570d2d8622e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('DeepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
